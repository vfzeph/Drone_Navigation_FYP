# Environment settings
environment:
  name: "CartPole-v1"
  state_size: 4
  action_size: 2
  seed: 42

# Agent settings
agent:
  type: "DQN" # Could be DQN, PPO, etc.
  gamma: 0.99
  learning_rate: 0.0005
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay: 0.995
  tau: 1e-3
  update_every: 4
  seed: 42

# Model architecture (for neural network-based agents like DQN)
model:
  hidden_layers: [64, 64]

# Memory settings (for agents using Replay Buffer)
memory:
  buffer_size: 100000
  batch_size: 64
  alpha: 0.6 # For prioritized experience replay
  beta: 0.4
  beta_increment_per_sampling: 0.001

# Training settings
training:
  n_episodes: 2000
  max_t: 1000
  solve_score: 195.0
  checkpoint_path: "checkpoints/model.pth"

# Logging settings
logging:
  log_interval: 100
  log_dir: "logs"
  tensorboard: True

# Optional: Settings specific to algorithms like PPO
ppo:
  lr_actor: 1e-4
  lr_critic: 2e-4
  eps_clip: 0.2
  K_epochs: 4
